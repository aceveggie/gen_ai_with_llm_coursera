# gen_ai_with_llm_coursera


## Syllabus:

1. Week 1: 
    * Introduction to LLMs and the generative AI project lifecycle:
        1. Prompt
        2. LLM
        3. Context Window.
        4. Complettion
        5. Inference
        6. Issues with Tradtional Text Generation (with LSTMs).
        7. Transformers
        8. Tokenizers
        9. Embedding Layers, Positional Embedding.
        10. Generating text with Transformers.
        11. Encoder Decoder models (BART, T5, etc.), Encoder only models (BERT, etc.), Decoder only models (GPT, LLAMA).
        12. Prompting, Prompt Engineering (In context learning, zero-shot inference, 1-shot inference, few shot inference).
        13. Generative Configuration (Max new tokens, Temperature, Greedy/Random sampling, Top-P, Top-K sampling).
        14. Gen AI Project Life Cycle.

    * LLM pre-training and scaling laws:
        1. Encoder only models, use cases, examples.
        2. Decoder only models, use cases, examples.
        3. Encoder Decoder models, use cases, examples.
        4. Computational challenges of training LLMs.
        5. Scaling laws and compute optimal models (Chinchilla).
        6. Pre-training for domain adaptation.
    
    * Quiz covering above 
    * Programming assignment on SageMaker:
        1. Set up Kernel and Required Dependencies.
        2. Summarize Dialogue without Prompt Engineering.
        3. Summarize Dialogue with an Instruction Prompt.
            1. Zero Shot Inference with an Instruction Prompt.
            2. Zero Shot Inference with the Prompt Template from FLAN-T5.
        4. Summarize Dialogue with One Shot and Few Shot Inference.
            1. One Shot Inference.
            2. Few Shot Inference.
        5. Generative Configuration Parameters for Inference.

        